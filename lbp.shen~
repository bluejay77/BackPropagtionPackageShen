\*

Dr AJY 2019-01-28 --


TODO: 0. modules

1.  bias

2.  momentum constant

3.  final checks


Phase #1.  The bias.

The weights are trainable w/ the back propagation.

One of the inputs of each and every neuron is identically constant,
the (value *BIAS*).  It is usually +1 or -1, in different references.

Modify all of the mathematics accordingly, with a pen and paper.

No types.  Plenty of work, not the corresponding advantage.

Handling the biases:

In the file obarray.shen there is there the constant *MAXVEC*, which
gives the max number of numbers in the inputs and the outputs vectors.
Those vectors are actually *MAX* == (+ 1 *MAXVEC*) items long.  The
input item #MAX is the (value *BIAS*), and the weight #MAX is the
trainable weight of the bias, which bias is a constant.

*\



\* Master file to load the backpropagation program files

(C) Dr A. J. Y. 2018-01-16 --

The multilayer perceptron with the error backpropagation algorithm
(Winston, 1992), (Haykin, 1999), (Luger, 2002), (Luger, 2009), (ICNN,
1994), (IEEE, 2005)

The above references:

(Winston, 1992) Patrick Henry Winston: Artificial Intelligence, 3rd
edition Addison-Wesley 1992, ISBN 0-201-53377-4

(Haykin, 1999) Simon Haykin: NEURAL NETWORKS, A Comprehensive
Foundation the author has the 2nd edition Prentice--Hall 1999, ISBN
0-13-273350-1.  and that the reader delve into Chapters 1--4.

(Luger, 2002) George F Luger: Artificial Intelligence,
Pearson--Addison-Wesley, the author has the 4th edition, ISBN
0-201-64866-0.
  
(Luger, 2009) George F. Luger: ARTIFICIAL INTELLIGENCE, 6th Edition;
Pearson--Addison-Wesley 2009, ISBN 978-0-13-209001-8.

(ICNN, 1994)

(IEEE, 2005) See http://ieeexplore.ieee.org/document/1007668/

Learning by the backpropagation algorithm:
Allen B Tucker: Comp Sci Handbook ISBN 1-58488-360-X p. 66-11
conjugate gradients
quasi-Newton (variable metric)
sum-of-squares cost function: Levenberg-Marquardt algorithm

The ANN example here is from the (Winston, 1992) from Pages 458--465.
The author has made two changes to the Winston example: The ANN
neurons have biases, as it is customary in the ANN applications.  And
moreover, we use the momentum constant, as in (Haykin, 1999) and
(ICNN, 1994).

(Anastasiadis, 2005) resilient backpropagation formulas
And the Wolfe condtions, from the literature

Various variants of the Resilient Propagation



---Algorithm:

(Haykin, 1999) pp. 173--175.

Phase 1.  Initialization.

Pick synaptic weights and thresholds from a uniform distribution,
whose mean == 0, and whose variance is given as a function parameter.
See (Haykin, 1999) p. 174.

Iterate UNITL (stopping criterion):

    Phase 2.  Present (assign) the training examples.  For this epoch.

    Phase 3.  Forward computation.  Self explanatory.

    Phase 4.  Backward Computation.  Compute the betas, deltas, sumDeltas.

END UNTIL

The order of presentation of training examples should be randomized
from epoch to epoch.

Four epoch iteration stopping criteria:

1.  Number of epochs given, e. g. (bprop 200)

2.  (Winston, 1992) <0.1 or >0.9 criterion, e. g. (cbprop)

3.  (Haykin, 1999) p. 173 criterion -- Euclidean norm of the gradient
of the error surface is sufficiently small, and is given as a function
argument (h1bprop 0.01)

4.  (Haykin, 1999) p. 173 criterion 2: absolute rate of change in the
average squared error per epoch is sufficiently small, is given as a
function parameter, (h2bprop 0.001)

The ones 3. and 4. have been left here for further development.

Use the sequential mode: Update the weights after the presentation of
each training example.  (Haykin, 1999) p. 171-172.

Apply the (Anastasiadis, 2005) resilient backpropagation algorithm.

*\



(do
    (load "maths-lib.shen")  \\ AJY 2018-05-15 -- 
    (load "bputp.shen")      \\ AJY 2018-05-15
    \* ***** Commented out, until tested!
    (load "obarray.shen")    \\ AJY 2018-05-15 --- 2018-06-07
    (load "random.shen")     \\ AJY 2018-05-15 ---
    (load "random-weight.shen") \\ AJY 2018-06-23 ---
    (load "record.shen")     \\ AJY 2018-05-15 ---
    (load "defneuron.shen")  \\ AJY 2018-05-15 --- 2018-05-16
    (load "aux1.shen")       \\ AJY 2018-05-16 ---
    (load "xfer.shen")       \\ AJY 2018-05-16 ---
    (load "for-all.shen")    \\ AJY 2018-05-21 ---
    (load "zero.shen")       \\ AJY 2018-07-07 ---
    (load "train.shen")      \\ AJY 2018-06-16 ---
    (load "assign.shen")     \\ AJY 2018-06-17 ---
    (load "launch.shen")     \\ AJY 2018-06-17 ---
    (load "b-out.shen")      \\ AJY 2018-06-17 ---
    (load "b-l.shen")        \\ AJY 2018-06-18 ---
    (load "d-1.shen")        \\ AJY 2018-06-18 ---
    (load "d-n.shen")        \\ AJY 2018-06-18 ---
    (load "change.shen")     \\ AJY 2018-06-18 ---
    (load "bapp.shen")       \\ AJY 2018-05-15 ---
    *\
    []
    )

\*

Do exemplaris gratis:

(load "lbp.shen")

and then:

(time (bprop 100))
or
(cbprop)

(dsp)
(getp *A* pdoc) \\ etc.....
(show-samples)


Exercise #1 to the reader: Carry out the Exercise 10.8.3. from (Luger,
2002) on Page 468.  Note that the backpropagation net in Figure 10.12
on Page 437, Luger, has a format slightly different from the ordinary
multilevel backpropagation ANN, and therefore some nontrivial
programming by the reader is required.

Exercise #2 to the reader: Program the well-known "Learn the XOR
Function Problem", with an ordinary multilevel backpropagation net.
How many layers (strata) will the reader need?  This one occurs
exemplaris gratis in (Haykin, 1999).

Exercise #3 for the reader: Do the Example Fig 7.11 p. 317 from the
Poole-Mackworth book ARTIFICIAL INTELLIGENCE, Cambridge: One more ANN
from the literature.

*\


